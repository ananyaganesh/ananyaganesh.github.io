<!DOCTYPE html>
<html lang="en">
<head>
    <meta name=viewport content=“width=800”>
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* Color scheme stolen from Sergey Karayev */
        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus, a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body, td, th, tr, p, a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 20px;
        }

        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
            font-weight: 700
        }

        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }

        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        span.highlight {
            background-color: #ffffd0;
        }
    </style>
    <link rel="icon" type="image/png" href="umass.jpg">
    <title>Projects</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
          type='text/css'>
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
</head>
<body>
<table width="100%" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
        <td>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <!--<td width="20%" valign="middle">-->
                    <!--<p>-->
                    <!--<img src="lisa.PNG" height="200" width="200">-->
                    <!--</p>-->
                    <!--</td>-->
                    <td width="60%">
                        <p>
                            <a name="LISA-SRL">
                                <heading>Improved representation learning for semantic role labeling
                                </heading>
                            </a>
                        </p>
                        <p align="left">
                            Semantic role labeling (SRL) is a shallow semantic parsing task that helps determine
                            <i>who</i> did <i>what</i> to <i>whom</i> at <i>where</i> by recovering the latent
                            predicate-argument structure of the sentence. SRL is a fundamental problem in NLP that is
                            also useful in applications such as question answering, machine translation and information
                            extraction. I've been working on reducing labeling errors on a state-of-the art SRL model
                            called Linguistically-Informed Self Attention (<a href="https://arxiv.org/abs/1804.08199">LISA</a>)
                            developed by <a href="http://people.cs.umass.edu/~strubell">Emma Strubell</a>. LISA is a
                            neural network model that performs multi-task learning across dependency parsing,
                            part-of-speech tagging, predicate detection and SRL.
                        </p>
                        <p align="left">

                            Error analysis on the model revealed that if labeling errors alone were fixed, the score
                            would improve by 5.8 absolute F1. That is, the predicates and arguments were identified
                            correctly in several cases, but classified wrong. My analysis further showed that 31% of the
                            labeling errors were due to core argument confusion in the <a
                                href="https://propbank.github.io/">PropBank</a> label set (verb-specific
                            roles from ARG0-ARG5). While PropBank is a useful semantic formalism, it defines
                            coarse-grained labels which aren't strictly associated with a role. Since the meaning of a
                            role changes across different predicates, it can be difficult for the model to learn these
                            roles. To fix this, we're augmenting PropBank labels with finer-grained <a
                                href="https://verbs.colorado.edu/~mpalmer/projects/verbnet.html">VerbNet</a> labels by
                            first predicting VerbNet roles and using the predictions to compose an auxiliary role
                            representation which is then utilized for PropBank SRL.
                        </p>

                    </td>
                    <td width="40%" valign="middle">
                        Jun 2018 - Present
                    </td>
                </tr>
                <tr>
                    <!--<td width="20%" valign="middle">-->
                    <!--<p>-->
                    <!--<img src="wsi.jpg" height="200" width="200">-->
                    <!--</p>-->
                    <!--</td>-->
                    <td width="60%">
                        <p>
                            <heading>Efficient Graph-Based Word Sense Induction</heading>
                        </p>
                        <p>
                            <a href="https://arxiv.org/abs/1804.03257">arxiv</a> | <a href="poster_wsi.pdf">poster</a>
                        </p>
                        <p align="justify">
                            This project was undertaken with the hypothesis that resolving polysemy would help improve
                            sentiment analysis . Polysemy is the phenomenon of a
                            single word having multiple senses, like <i>bank</i> the financial institution and
                            <i>bank</i> as in river bank. The task of selecting the right sense is called word sense
                            disambiguation, while the unsupervised discovery of latent senses is called word sense
                            induction (WSI). We developed an efficient method to perform word sense
                            induction using graph-based clustering.
                        </p>
                        <p>
                            Typically, graph-based clustering methods for WSI construct an 'ego-network' by finding the
                            nearest neighbors of the target word in the word-embedding space. However, this can be
                            computationally expensive if the graph is large, so we instead proposed to group words into
                            basis indexes that resemble topics, and then construct a graph in which each node is a basis
                            index relevant to the topic word. To obtain these basis indexes, we make use of
                            Distributional Inclusion Vector Embeddings(<a
                                href="https://arxiv.org/abs/1710.00880">DIVE</a>) developed by <a
                                href="http://people.umass.edu/hawshiuancha/">Haw-Shiuan Chang</a>.
                            Sense clusters are
                            then obtained by clustering basis indexes using spectral clustering. We represent each sense
                            cluster by a sense embedding, which is the average of the topic embeddings in the cluster,
                            weighted by its relevance to the target word.
                        </p>
                        <p>
                            We then perform expectation-maximization to
                            refine the sense embeddings, where the E-step is replacing all words in the corpus with the
                            sense it represents, and the M-step is to retrain word embeddings using the induced senses.
                            Our method beats the previous state-of-the-art on several word context relevance tasks while
                            producing more interpretable sense clusters more efficiently. While we haven't yet been able
                            to correlate better word sense disambiguation with improvement in sentiment analysis, we
                            plan to get back to this task in the near future.
                        </p>

                    </td>
                    <td width="40%">
                        Feb 2018 - Apr 2018
                    </td>
                </tr>
                <!--<tr>-->
                <!--<td width="20%" valign="middle">-->
                <!--<p>-->
                <!--<img src="dil.PNG" height="200" width="200">-->
                <!--</p>-->
                <!--</td>-->
                <!--<td width="50%" valign="middle">-->
                <!--<p>-->
                <!--<heading>Faster dependency parsing using iterative dilated convolutions</heading>-->
                <!--</p>-->
                <!--<p>-->
                <!--Iterative dilated CNNs are a stack of dilated convolutional layers, found to be effective-->
                <!--for-->
                <!--fast sequence labeling. I worked on integrating ID-CNNs into the LISA model-->
                <!--as a replacement for the LSTM.-->
                <!--</p>-->
                <!--</td>-->
                <!--<td width="30%">-->
                <!--Mar 2018 - Present?-->
                <!--</td>-->
                <!--</tr>-->
                <tr>
                    <td width="20%" valign="middle">
                        <p>
                            <img src="low-shot.PNG" height="200" width="200">
                        </p>
                    </td>
                    <td width="50%" valign="middle">
                        <p>
                            <heading>Low-shot visual recognition for faces</heading>
                        </p>
                        <p>
                            <a href="Final%20report.pdf">report</a> | <a
                                href="http://github.com/ananyaganesh/low-shot-face">code</a>
                        </p>
                        <p>
                            Low shot learning, or the ability to learn from a small
                            number of examples, is a relevant problem in the domain of
                            facial recognition, where access to training data is limited by cost and privacy issues
                            We explored a solution based on data augmentation by hallucinating
                            new examples, which was <a href="https://arxiv.org/abs/1606.02819">found to work well</a>
                            for
                            classification on ImageNet by researchers at Facebook AI Research. We used a subset of <a
                                href="https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/">MS-Celeb-1</a>
                            for training data.
                        </p>
                        <p>
                            In the low-shot
                            learning set-up, there are a fixed number of base classes, for
                            which a large number of training examples are available,
                            and then there are novel classes, for which a limited number
                            of training examples are available. The classifier is then evaluated based on its ability to
                            correctly classify both the base and novel classes. Our data augmentation method creates new
                            examples for the novel classes in the following way: The features of each base class are
                            grouped
                            into clusters using K-means clustering. The difference between two clusters can be
                            considered a transformation, such as a front-facing image to a side-facing image. All
                            transformations are mined from all base classes and used to train a generator. Then, for
                            each image in the novel class, a set of transformations are applied on the base class to
                            generate new examples. We compare to a baseline where images are generated by naive
                            approaches such as jittering, and achieve a significant improvement.
                        </p>
                    </td>
                    <td width="30%">
                        Mar 2018 - Apr 2018
                    </td>
                </tr>
                <tr>
                    <td width="20%" valign="middle">
                        <p>
                            <img src="geo.PNG" height="200" width="200">
                        </p>
                    </td>
                    <td width="50%">
                        <p>
                            <heading>Visual Place Recognition</heading>
                        </p>
                        <p>
                            As a course project for Computer Vision, I worked on automatically identifying the location
                            of a
                            place given only an image and no other metadata.
                        </p>
                    </td>
                    <td width="30%">
                        Oct 2017 - Dec 2017
                    </td>
                </tr>
                <tr>
                    <td width="20%" valign="middle">
                        <p>
                            <img src="siren.jpg">
                        </p>
                    </td>
                    <td width="50%" valign="middle">
                        <p>
                            <heading>The Sound of Sirens</heading>
                        </p>
                        <p>
                            This was a project that I worked on over 36 hours at HackUMass 2017 along with some cool
                            undergrads that I met at the venue. We built a signaling system that could alert hearing
                            impaired drivers if a vehicle with sirens is in the vicinity.
                        </p>
                    </td>
                    <td width="30%">
                        Nov 2017 - Nov 2017
                    </td>
                </tr>
                <tr>
                    <td width="20%" valign="middle">
                        <p>
                            <img src="quoterequest.png">
                        </p>
                    </td>
                    <td width="50%" valign="middle">
                        <p>
                            <heading>CEGAR-based tool for specifying system properties</heading>
                        </p>
                        <p>
                            This was my bachelor's thesis in the field of model checking.
                        </p>
                    </td>
                    <td width="30%">
                        Jan 2017 - May 2017
                    </td>
                </tr>
                <tr>
                    <td width="20%" valign="middle">
                        <p>
                            <img src="siren.jpg">
                        </p>
                    </td>
                    <td width="70%" valign="middle">
                        <p>
                            <heading>Detecting variability in multi-word expressions</heading>
                        </p>
                        <p>
                            I worked as a research intern at the Computational Linguistics lab at Nara Institute of
                            Science
                            and Technology in Japan.
                        </p>
                    </td>
                    <td width="30%">
                        Jun 2016 - Jul 2016
                    </td>
                </tr>
                <tr>
                    <td width="20%" valign="middle">
                        <p>
                            <img src="bull.jpg">
                        </p>
                    </td>
                    <td width="70%" valign="middle">
                        <p>
                            <heading>Sentiment analysis for foreign exchange trading</heading>
                        </p>
                        <p>
                            As a data science intern at Serendio Inc., I worked on developing a machine learning model
                            to
                            gauge expert opinion on trends in currency exchange using sentiment analysis.
                        </p>
                    </td>
                    <td width="30%">
                        Jan 2016 - Feb 2016
                    </td>
                </tr>

            </table>


            <script type="text/javascript">
                var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
                document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

            </script>
            <script type="text/javascript">
                try {
                    var pageTracker = _gat._getTracker("UA-7580334-1");
                    pageTracker._trackPageview();
                } catch (err) {
                }
            </script>
        </td>
    </tr>
</table>
</body>
</html>