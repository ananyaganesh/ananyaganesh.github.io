<!DOCTYPE html>
<html lang="en">
<head>
    <meta name=viewport content=“width=800”>
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* Color scheme stolen from Sergey Karayev */
        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus, a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body, td, th, tr, p, a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 20px;
        }

        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
            font-weight: 700
        }

        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }

        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        span.highlight {
            background-color: #ffffd0;
        }
    </style>
    <link rel="icon" type="image/png" href="umass.jpg">
    <title>Projects</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
          type='text/css'>
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
</head>
<body>
<table width="100%" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
        <td>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <!--<td width="20%" valign="middle">-->
                    <!--<p>-->
                    <!--<img src="lisa.PNG" height="200" width="200">-->
                    <!--</p>-->
                    <!--</td>-->
                    <td width="60%">
                        <p>
                            <a name="LISA-SRL">
                                <heading>Improved representation learning for semantic role labeling
                                </heading>
                            </a>
                        </p>
                        <p align="left">
                            Semantic role labeling (SRL) is a shallow semantic parsing task that helps determine
                            <i>who</i> did <i>what</i> to <i>whom</i> at <i>where</i> by recovering the latent
                            predicate-argument structure of the sentence. SRL is a fundamental problem in NLP that is
                            also useful in applications such as question answering, machine translation and information
                            extraction. I've been working on reducing labeling errors on a state-of-the art SRL model
                            called Linguistically-Informed Self Attention (<a href="https://arxiv.org/abs/1804.08199">LISA</a>)
                            developed by <a href="http://people.cs.umass.edu/~strubell">Emma Strubell</a>. LISA is a
                            neural network model that performs multi-task learning across dependency parsing,
                            part-of-speech tagging, predicate detection and SRL.
                        </p>
                        <p align="left">

                            Error analysis on the model revealed that if labeling errors alone were fixed, the score
                            would improve by 5.8 absolute F1. That is, the predicates and arguments were identified
                            correctly in several cases, but classified wrong. My analysis further showed that 31% of the
                            labeling errors were due to core argument confusion in the <a
                                href="https://propbank.github.io/">PropBank</a> label set (verb-specific
                            roles from ARG0-ARG5). While PropBank is a useful semantic formalism, it defines
                            coarse-grained labels which aren't strictly associated with a role. Since the meaning of a
                            role changes across different predicates, it can be difficult for the model to learn these
                            roles. To fix this, we're augmenting PropBank labels with finer-grained <a
                                href="https://verbs.colorado.edu/~mpalmer/projects/verbnet.html">VerbNet</a> labels by
                            first predicting VerbNet roles and using the predictions to compose an auxiliary role
                            representation which is then utilized for PropBank SRL.
                        </p>

                    </td>
                    <td width="40%" valign="middle">
                        Jun 2018 - Present
                    </td>
                </tr>
                <tr>
                    <!--<td width="20%" valign="middle">-->
                    <!--<p>-->
                    <!--<img src="wsi.jpg" height="200" width="200">-->
                    <!--</p>-->
                    <!--</td>-->
                    <td width="60%">
                        <p>
                            <heading>Efficient Graph-Based Word Sense Induction</heading>
                        </p>
                        <p>
                            <a href="https://arxiv.org/abs/1804.03257">arxiv</a> | <a href="poster_wsi.pdf">poster</a>
                        </p>
                        <p align="justify">
                            This project was undertaken with the hypothesis that resolving polysemy would help improve
                            sentiment analysis . Polysemy is the phenomenon of a
                            single word having multiple senses, like <i>bank</i> the financial institution and
                            <i>bank</i> as in river bank. The task of selecting the right sense is called word sense
                            disambiguation, while the unsupervised discovery of latent senses is called word sense
                            induction (WSI). We developed an efficient method to perform word sense
                            induction using graph-based clustering.
                        </p>
                        <p>
                            Typically, graph-based clustering methods for WSI construct an 'ego-network' by finding the
                            nearest neighbors of the target word in the word-embedding space. However, this can be
                            computationally expensive if the graph is large, so we instead proposed to group words into
                            basis indexes that resemble topics, and then construct a graph in which each node is a basis
                            index relevant to the topic word. To obtain these basis indexes, we make use of
                            Distributional Inclusion Vector Embeddings(<a
                                href="https://arxiv.org/abs/1710.00880">DIVE</a>) developed by <a
                                href="http://people.umass.edu/hawshiuancha/">Haw-Shiuan Chang</a>.
                            Sense clusters are
                            then obtained by clustering basis indexes using spectral clustering. We represent each sense
                            cluster by a sense embedding, which is the average of the topic embeddings in the cluster,
                            weighted by its relevance to the target word.
                        </p>
                        <p>
                            We then perform expectation-maximization to
                            refine the sense embeddings, where the E-step is replacing all words in the corpus with the
                            sense it represents, and the M-step is to retrain word embeddings using the induced senses.
                            Our method beats the previous state-of-the-art on several word context relevance tasks while
                            producing more interpretable sense clusters more efficiently. While we haven't yet been able
                            to correlate better word sense disambiguation with improvement in sentiment analysis, we
                            plan to get back to this task in the near future.
                        </p>

                    </td>
                    <td width="40%">
                        Feb 2018 - Apr 2018
                    </td>
                </tr>
                <!--<tr>-->
                <!--<td width="20%" valign="middle">-->
                <!--<p>-->
                <!--<img src="dil.PNG" height="200" width="200">-->
                <!--</p>-->
                <!--</td>-->
                <!--<td width="50%" valign="middle">-->
                <!--<p>-->
                <!--<heading>Faster dependency parsing using iterative dilated convolutions</heading>-->
                <!--</p>-->
                <!--<p>-->
                <!--Iterative dilated CNNs are a stack of dilated convolutional layers, found to be effective-->
                <!--for-->
                <!--fast sequence labeling. I worked on integrating ID-CNNs into the LISA model-->
                <!--as a replacement for the LSTM.-->
                <!--</p>-->
                <!--</td>-->
                <!--<td width="30%">-->
                <!--Mar 2018 - Present?-->
                <!--</td>-->
                <!--</tr>-->
                <tr>
                    <!--<td width="20%" valign="middle">-->
                    <!--<p>-->
                    <!--<img src="low-shot.PNG" height="200" width="200">-->
                    <!--</p>-->
                    <!--</td>-->
                    <td width="60%">
                        <p>
                            <heading>Low-shot visual recognition for faces</heading>
                        </p>
                        <p>
                            <a href="Final%20report.pdf">report</a> | <a
                                href="http://github.com/ananyaganesh/low-shot-face">code</a>
                        </p>
                        <p>
                            Low shot learning, or the ability to learn from a small
                            number of examples, is a relevant problem in the domain of
                            facial recognition, where access to training data is limited by cost and privacy issues
                            We explored a solution based on data augmentation by hallucinating
                            new examples, which was <a href="https://arxiv.org/abs/1606.02819">found to work well</a>
                            for
                            classification on ImageNet by researchers at Facebook AI Research. We used a subset of <a
                                href="https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/">MS-Celeb-1</a>
                            for training data.
                        </p>
                        <p>
                            In the low-shot
                            learning set-up, there are a fixed number of base classes, for
                            which a large number of training examples are available,
                            and then there are novel classes, for which a limited number
                            of training examples are available. The classifier is then evaluated based on its ability to
                            correctly classify both the base and novel classes. Our data augmentation method creates new
                            examples for the novel classes in the following way: The features of each base class are
                            grouped
                            into clusters using K-means clustering. The difference between two clusters can be
                            considered a transformation, such as a front-facing image to a side-facing image. All
                            transformations are mined from all base classes and used to train a generator. Then, for
                            each image in the novel class, a set of transformations are applied on the base class to
                            generate new examples. We compare to a baseline where images are generated by naive
                            approaches such as jittering, and achieve a significant improvement.
                        </p>
                    </td>
                    <td width="40%">
                        Mar 2018 - Apr 2018
                    </td>
                </tr>
                <tr>
                    <!--<td width="20%" valign="middle">-->
                    <!--<p>-->
                    <!--<img src="geo.PNG" height="200" width="200">-->
                    <!--</p>-->
                    <!--</td>-->
                    <td width="60%">
                        <p>
                            <heading>Visual Place Recognition</heading>
                        </p>
                        <p>
                            <a href="Report">report</a>
                        </p>
                        <p>
                            As a course project for Computer Vision, I worked on automatically identifying the location
                            of a place given only an image and no other metadata. We compared three
                            machine learning models which use different kinds of features
                            and evaluate their suitability for this task. The first
                            approach uses a color histogram to represent the image; the
                            second approach uses the GIST global descriptor as image
                            features; the third approach uses raw images with a convolutional
                            neural network without explicitly extracting features. We used the <a
                                href="http://crcv.ucf.edu/data/GMCP_Geolocalization/">Google Street View dataset</a>,
                            which contains 62058 images from three cities: Pittsburgh, Orlando, and New York.
                        </p>
                    </td>
                    <td width="40%">
                        Oct 2017 - Dec 2017
                    </td>
                </tr>
                <tr>
                    <!--<td width="20%" valign="middle">-->
                    <!--<p>-->
                    <!--<img src="siren.jpg">-->
                    <!--</p>-->
                    <!--</td>-->
                    <td width="60%">
                        <p>
                            <heading>The Sound of Sirens</heading>
                        </p>
                        <p>
                            This was a project that I worked on over 36 hours at HackUMass 2017 along with some cool
                            undergrads that I met at the venue. We built a signaling system that could alert hearing
                            impaired drivers if a vehicle with sirens is in the vicinity. To detect sirens, we trained a
                            neural network on the UrbanSounds dataset, by extracting features such as the short-time
                            Fourier transform, mel spectogram, and contrast. Our model achieved a test time accuracy of
                            over 91%. We then interfaced with a Myo wristband using the Myo-SDK to make the wristband
                            vibrate every time a siren was detected.
                        </p>
                    </td>
                    <td width="40%">
                        Nov 2017 - Nov 2017
                    </td>
                </tr>
                <tr>
                    <!--<td width="20%" valign="middle">-->
                    <!--<p>-->
                    <!--<img src="quoterequest.png">-->
                    <!--</p>-->
                    <!--</td>-->
                    <td width="60%">
                        <p>
                            <heading>CEGAR-based tool for specifying system properties</heading>
                        </p>
                        <p>
                            This was my bachelor's thesis in the field of model checking.
                        </p>
                    </td>
                    <td width="40%">
                        Jan 2017 - May 2017
                    </td>
                </tr>
                <tr>
                    <!--<td width="20%" valign="middle">-->
                    <!--<p>-->
                    <!--<img src="siren.jpg">-->
                    <!--</p>-->
                    <!--</td>-->
                    <td width="60%">
                        <p>
                            <heading>Detecting variability in multi-word expressions</heading>
                        </p>
                        <p>
                            As a research intern at the <a href="https://cl.naist.jp/en/">Computational
                            Linguistics Lab</a> of Nara Institute of
                            Science and Technology, Japan, I worked on multi-word expressions (MWE) with Professor <a
                                href=" http://cl.naist.jp/staff/matsu/home-e.html">Yuji Matsumoto</a>. MWEs are made up
                            of two or more words but tend to act as a single lexical unit, such as <i>by the way</i>.
                            While the above expression is fixed, and always occurs exactly in one form, some MWEs may be
                            flexible and thus harder to deal with, such as <i>under the circumstances</i> occurring as
                            <i>under the specific circumstances</i>. My project was to
                            automatically detect flexible-type multi word expressions (MWE) in English, and compile them
                            into a dictionary to enable MWE-aware POS-tagging.
                        </p>
                        <p>
                            Starting with a candidate list of over 2600 MWEs, I implemented a rule based system to
                            detect occurrences of each MWE and its possible variations in the LDC GigaWord corpus. The
                            initial
                            rules were as follows: allow up to two intervening words at all positions in the MWE (for
                            the MWE <i>a number of</i>, also look for <i>a large number of</i> and <i>a very large
                            number of</i>), allow interchangeable articles and pronouns (<i>apple of his/her/their
                            eye</i>), allow plural forms of nouns, and tense variations in verbs, etc. I then counted
                            the usage of the original MWE in comparison with its modified versions and imposed a
                            threshold to classify each as fixed or flexible, as well as to prune rules that didn't work
                            well.
                        </p>
                    </td>
                    <td width="40%">
                        Jun 2016 - Jul 2016
                    </td>
                </tr>
                <tr>
                    <!--<td width="20%" valign="middle">-->
                    <!--<p>-->
                    <!--<img src="bull.jpg">-->
                    <!--</p>-->
                    <!--</td>-->
                    <td width="60%">
                        <p>
                            <heading>Sentiment analysis for foreign exchange trading</heading>
                        </p>
                        <p>
                            As a data science intern at Serendio Inc., I worked on developing a machine learning model
                            to gauge expert opinion on trends in currency exchange using sentiment analysis. The task
                            was to predict whether the sentiment about a specific currency pair, such as USD/EUR was
                            bullish (favors buying) or bearish (favors selling) based on posts on financial forums such
                            as Bloomberg, moneycontrol, etc. To gather training data, we scraped posts from StockTwits,
                            a forum on which users post their opinion on stock market trends and tag them as bullish or
                            bearish. We then built an ensemble of multiple machine learning models to do sentiment
                            classification. Specifically, I implemented a Naive-Bayes classifier which achieved an
                            accuracy of over 94% on the validation set.
                        </p>
                    </td>
                    <td width="40%">
                        Jan 2016 - Feb 2016
                    </td>
                </tr>

            </table>


            <script type="text/javascript">
                var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
                document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

            </script>
            <script type="text/javascript">
                try {
                    var pageTracker = _gat._getTracker("UA-7580334-1");
                    pageTracker._trackPageview();
                } catch (err) {
                }
            </script>
        </td>
    </tr>
</table>
</body>
</html>